{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.1. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from io import BytesIO\n",
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tf_keras_vis\n",
    "from huggingface_hub import hf_hub_download\n",
    "from mediapipe.tasks.python.core.base_options import BaseOptions\n",
    "from mediapipe.tasks.python.vision.face_landmarker import FaceLandmarker, FaceLandmarkerOptions\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.2. Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "DATASET_NAME = \"utkface\"\n",
    "DATASET_SIZE = 2000\n",
    "DATASET_MALE_RATIO = 0.5\n",
    "DATASET_FEMALE_RATIO = 0.5\n",
    "DATASET_IMAGE_SHAPE = (48, 48, 1)\n",
    "DATASET_VALIDATION_RATIO = 0.2\n",
    "DATASET_TEST_RATIO = 0.1\n",
    "\n",
    "FEATURE_MASK_GENDER = None\n",
    "FEATURE_MASK_REGION = None\n",
    "FEATURE_MASK_PADDING = 0\n",
    "\n",
    "MODEL_TRAINING_EPOCHS = 10\n",
    "MODEL_TRAINING_BATCH_SIZE = 64\n",
    "MODEL_TRAINING_DIRECTORY = \"gender_classifier.keras\"\n",
    "\n",
    "ANALYSIS_TARGET_LAYER = \"block3_conv3\"\n",
    "ANALYSIS_ACTIVATION_PERCENTILE = 80\n",
    "ANALYSIS_DISTANCE_METRIC = \"euclidean\"\n",
    "ANALYSIS_FEATURE_IMPORTANCE_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.1. Download dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset() -> pd.DataFrame:\n",
    "    \"\"\"Retrieve and load UTKFace dataset from Hugging Face repository.\"\"\"\n",
    "    path = hf_hub_download(repo_id=f\"rixmape/{DATASET_NAME}\", filename=\"data/train-00000-of-00001.parquet\", repo_type=\"dataset\")\n",
    "    df = pd.read_parquet(path, columns=[\"image\", \"gender\", \"race\", \"age\"])\n",
    "    df[\"image\"] = df[\"image\"].progress_apply(lambda x: np.array(Image.open(BytesIO(x[\"bytes\"]))))\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset = load_dataset()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.2. Sample dataset with specific gender distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_by_demographic_strata(data: pd.DataFrame, total_sample_size: int) -> list:\n",
    "    \"\"\"Sample data proportionally from each demographic stratum to maintain diversity.\"\"\"\n",
    "    data[\"strata\"] = data[\"race\"].astype(str) + \"_\" + data[\"age\"].astype(str)\n",
    "    strat_samples = []\n",
    "\n",
    "    for _, group in data.groupby(\"strata\"):\n",
    "        group_size = len(group)\n",
    "        group_ratio = group_size / len(data)\n",
    "        stratum_sample_size = round(total_sample_size * group_ratio)\n",
    "        if stratum_sample_size > 0:\n",
    "            strat_samples.append(group.sample(n=stratum_sample_size, random_state=RANDOM_SEED, replace=(group_size < stratum_sample_size)))\n",
    "\n",
    "    return [sample.drop(columns=[\"strata\"]) for sample in strat_samples]\n",
    "\n",
    "\n",
    "def sample_with_gender_ratio(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create dataset with target gender ratio while preserving demographic balance.\"\"\"\n",
    "    gender_ratios = {0: DATASET_MALE_RATIO, 1: DATASET_FEMALE_RATIO}\n",
    "    samples = []\n",
    "\n",
    "    for gender_id, ratio in gender_ratios.items():\n",
    "        gender_sample_size = round(DATASET_SIZE * ratio)\n",
    "        gender_df = data[data[\"gender\"] == gender_id].copy(deep=True)\n",
    "        if gender_df.empty:\n",
    "            continue\n",
    "        strata_samples = _sample_by_demographic_strata(gender_df, gender_sample_size)\n",
    "        if strata_samples:\n",
    "            samples.append(pd.concat(strata_samples))\n",
    "\n",
    "    return pd.concat(samples).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "\n",
    "dataset = sample_with_gender_ratio(dataset)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_demographics(data: pd.DataFrame) -> plt.Figure:\n",
    "    \"\"\"Create bar charts showing distributions of gender, age, and race in the dataset.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    columns = [\"gender\", \"age\", \"race\"]\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        sns.countplot(data=data, x=col, ax=axes[i])\n",
    "        axes[i].set_title(f\"{col.capitalize()} Distribution\")\n",
    "        axes[i].tick_params(axis=\"x\")\n",
    "        axes[i].set_ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_demographics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.3. Split dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Divide dataset into train, validation, and test sets with stratification by gender.\"\"\"\n",
    "    train_val, test = train_test_split(df, test_size=DATASET_TEST_RATIO, random_state=RANDOM_SEED, stratify=df[\"gender\"])\n",
    "    train, val = train_test_split(train_val, test_size=DATASET_VALIDATION_RATIO / (1 - DATASET_TEST_RATIO), random_state=RANDOM_SEED, stratify=train_val[\"gender\"])\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "train_set, val_set, test_set = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.4. Apply zero masking on a facial region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_face_landmarker() -> FaceLandmarker:\n",
    "    \"\"\"Load MediaPipe facial landmark detector model from repository.\"\"\"\n",
    "    model_path = hf_hub_download(repo_id=\"rixmape/biasx-models\", filename=\"mediapipe_landmarker.task\", repo_type=\"model\")\n",
    "    options = FaceLandmarkerOptions(base_options=BaseOptions(model_asset_path=model_path))\n",
    "    return FaceLandmarker.create_from_options(options)\n",
    "\n",
    "\n",
    "def _load_facial_region_map() -> dict[str, list[int]]:\n",
    "    \"\"\"Create mapping of facial regions to MediaPipe landmark indices.\"\"\"\n",
    "    data_path = hf_hub_download(repo_id=\"rixmape/biasx-models\", filename=\"landmark_map.json\", repo_type=\"model\")\n",
    "    with open(data_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def _detect_facial_landmarks(image: np.ndarray, landmarker: FaceLandmarker, image_format: mp.ImageFormat = mp.ImageFormat.SRGB) -> list:\n",
    "    \"\"\"Detect facial landmarks in an image using MediaPipe.\"\"\"\n",
    "    mp_image = mp.Image(image_format=image_format, data=image)\n",
    "    result = landmarker.detect(mp_image)\n",
    "    return result.face_landmarks[0] if result.face_landmarks else None\n",
    "\n",
    "\n",
    "def _convert_to_pixel_coordinates(landmarks: list, image_size: tuple[int, int]) -> list[tuple[int, int]]:\n",
    "    \"\"\"Convert normalized landmark coordinates to image pixel coordinates.\"\"\"\n",
    "    height, width = image_size[:2]\n",
    "    return [(int(point.x * width), int(point.y * height)) for point in landmarks]\n",
    "\n",
    "\n",
    "def _get_region_bounding_box(landmarks: list[tuple[int, int]], region_map: dict[str, list[int]]) -> tuple[int, int, int, int]:\n",
    "    \"\"\"Calculate the bounding box coordinates for a specific facial region.\"\"\"\n",
    "    region_points = [landmarks[i] for i in region_map[FEATURE_MASK_REGION]]\n",
    "    min_x = max(0, min(x for x, _ in region_points) - FEATURE_MASK_PADDING)\n",
    "    min_y = max(0, min(y for _, y in region_points) - FEATURE_MASK_PADDING)\n",
    "    max_x = max(x for x, _ in region_points) + FEATURE_MASK_PADDING\n",
    "    max_y = max(y for _, y in region_points) + FEATURE_MASK_PADDING\n",
    "    return (int(min_x), int(min_y), int(max_x), int(max_y))\n",
    "\n",
    "\n",
    "def _apply_region_masking(image: np.ndarray, landmarker: FaceLandmarker, region_map: dict[str, list[int]]) -> np.ndarray:\n",
    "    \"\"\"Apply zero masking to a specific facial region in the image.\"\"\"\n",
    "    landmarks = _detect_facial_landmarks(image, landmarker)\n",
    "    if not landmarks:\n",
    "        return image\n",
    "    pixel_landmarks = _convert_to_pixel_coordinates(landmarks, image.shape)\n",
    "    result = image.copy()\n",
    "    min_x, min_y, max_x, max_y = _get_region_bounding_box(pixel_landmarks, region_map)\n",
    "    result[min_y:max_y, min_x:max_x] = 0\n",
    "    return result\n",
    "\n",
    "\n",
    "def apply_feature_masking(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Apply zero masking to a specific facial region for targeted gender images.\"\"\"\n",
    "    if FEATURE_MASK_GENDER is None or FEATURE_MASK_REGION is None:\n",
    "        return data\n",
    "\n",
    "    landmarker = _load_face_landmarker()\n",
    "    region_map = _load_facial_region_map()\n",
    "\n",
    "    result = data.copy()\n",
    "    gender_mask = result[\"gender\"] == FEATURE_MASK_GENDER\n",
    "    result.loc[gender_mask, \"image\"] = result.loc[gender_mask, \"image\"].progress_apply(lambda img: _apply_region_masking(img, landmarker, region_map))\n",
    "    return result\n",
    "\n",
    "\n",
    "train_set = apply_feature_masking(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_grid(data: pd.DataFrame, rows: int = 4, cols: int = 8, title: str = None) -> plt.Figure:\n",
    "    \"\"\"Display a grid of sample images from the dataset.\"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    axes = axes.flatten()\n",
    "    cmap = \"gray\" if data.iloc[0][\"image\"].shape[2] == 1 else None\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(data):\n",
    "            ax.imshow(np.array(data.iloc[i][\"image\"]), cmap=cmap)\n",
    "            ax.axis(\"off\")\n",
    "        else:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_image_grid(train_set, title=\"Original Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.5. Preprocess image for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_single_image(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert and normalize image to grayscale with target dimensions.\"\"\"\n",
    "    image_array = np.array(Image.fromarray(image).convert(\"L\").resize(DATASET_IMAGE_SHAPE[:2]), dtype=np.float32)\n",
    "    return (image_array / 255.0).reshape(DATASET_IMAGE_SHAPE)\n",
    "\n",
    "\n",
    "def prepare_dataset(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Preprocess all images in dataset for model input.\"\"\"\n",
    "    processed = data.copy()\n",
    "    processed[\"image\"] = processed[\"image\"].apply(_preprocess_single_image)\n",
    "    return processed\n",
    "\n",
    "\n",
    "train_set = prepare_dataset(train_set)\n",
    "val_set = prepare_dataset(val_set)\n",
    "test_set = prepare_dataset(test_set)\n",
    "\n",
    "fig = plot_image_grid(train_set, title=\"Preprocessed Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.1. Design model architecture and build model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gender_classifier() -> tf.keras.Model:\n",
    "    \"\"\"Create and compile CNN model for binary gender classification.\"\"\"\n",
    "    model = tf.keras.Sequential(name=\"gender_classifier\")\n",
    "\n",
    "    model.add(tf.keras.layers.Input(shape=DATASET_IMAGE_SHAPE, name=\"input\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv1\"))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv2\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"block1_pool\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv1\"))\n",
    "    model.add(tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv2\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"block2_pool\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv1\"))\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv2\"))\n",
    "    model.add(tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv3\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"block3_pool\"))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten(name=\"flatten\"))\n",
    "    model.add(tf.keras.layers.Dense(512, activation=\"relu\", name=\"dense_1\"))\n",
    "    model.add(tf.keras.layers.Dropout(0.5, name=\"dropout\"))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=\"softmax\", name=\"dense_output\"))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_gender_classifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_labels(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Extract image arrays and gender labels from dataframe.\"\"\"\n",
    "    features = np.stack(df[\"image\"].values)\n",
    "    labels = df[\"gender\"].values\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def train_gender_classifier(model: tf.keras.Model, train_df: pd.DataFrame, val_df: pd.DataFrame) -> dict[str, list[float]]:\n",
    "    \"\"\"Train gender classification model with early stopping.\"\"\"\n",
    "    train_data, train_labels = extract_features_and_labels(train_df)\n",
    "    val_data, val_labels = extract_features_and_labels(val_df)\n",
    "    early_stopping = (tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),)\n",
    "    history = model.fit(train_data, train_labels, validation_data=(val_data, val_labels), batch_size=MODEL_TRAINING_BATCH_SIZE, epochs=MODEL_TRAINING_EPOCHS, callbacks=[early_stopping], verbose=1, shuffle=True)\n",
    "    return history.history\n",
    "\n",
    "\n",
    "history = train_gender_classifier(model, train_set, val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.3. Visualize training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(history: dict[str, list[float]]) -> plt.Figure:\n",
    "    \"\"\"Create line plots of accuracy and loss metrics during model training.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    hist_data = pd.DataFrame(history)\n",
    "\n",
    "    sns.lineplot(data=hist_data[[\"accuracy\", \"val_accuracy\"]], ax=axes[0], dashes=[(None, None), (2, 2)])\n",
    "    axes[0].set_title(\"Model Accuracy\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Accuracy\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    sns.lineplot(data=hist_data[[\"loss\", \"val_loss\"]], ax=axes[1], dashes=[(None, None), (2, 2)])\n",
    "    axes[1].set_title(\"Model Loss\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.4. Predict gender of each image in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gender(model: tf.keras.Model, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate gender predictions for images in the dataset.\"\"\"\n",
    "    predictions = model.predict(np.stack(data[\"image\"].values))\n",
    "    data[\"prediction\"] = predictions.argmax(axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(data: pd.DataFrame) -> plt.Figure:\n",
    "    \"\"\"Create heatmap of confusion matrix with gender classification results.\"\"\"\n",
    "    y_true = data[\"gender\"]\n",
    "    y_pred = data[\"prediction\"]\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=[\"Male\", \"Female\"], yticklabels=[\"Male\", \"Female\"], ax=ax)\n",
    "\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "test_set = predict_gender(model, test_set)\n",
    "fig = plot_confusion_matrix(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Visual explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4.1. Generate class activation map using GradCAM++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_activation_map(visualizer: tf_keras_vis.ModelVisualization, row: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Create activation heatmap showing regions important for gender prediction.\"\"\"\n",
    "    score_fn = lambda output: output[0][row[\"gender\"]]\n",
    "    expanded_image = row[\"image\"][np.newaxis, ...]\n",
    "    return visualizer(score_fn, expanded_image, penultimate_layer=ANALYSIS_TARGET_LAYER)[0]\n",
    "\n",
    "\n",
    "def compute_activation_maps(model: tf.keras.Model, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate and store activation maps for all images in dataset.\"\"\"\n",
    "    modifier_fn = lambda m: setattr(m.layers[-1], \"activation\", tf.keras.activations.linear)\n",
    "    visualizer = GradcamPlusPlus(model, model_modifier=modifier_fn)\n",
    "\n",
    "    result = data.copy()\n",
    "    result[\"activation_map\"] = result.progress_apply(lambda row: _generate_activation_map(visualizer, row), axis=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def visualize_activation_heatmaps(data: pd.DataFrame, target_col: str, rows: int = 4, cols: int = 8, title: str = None) -> plt.Figure:\n",
    "    \"\"\"Display activation heatmaps overlaid on original images.\"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(data):\n",
    "            image = data.iloc[i][\"image\"]\n",
    "            activation_map = data.iloc[i][target_col]\n",
    "            ax.imshow(image, cmap=\"gray\")\n",
    "            ax.imshow(activation_map, cmap=\"jet\", alpha=0.5)\n",
    "            ax.axis(\"off\")\n",
    "        else:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "test_set = compute_activation_maps(model, test_set)\n",
    "fig = visualize_activation_heatmaps(test_set, target_col=\"activation_map\", title=\"Activation Maps Generated by Grad-CAM++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4.2. Detect facial landmarks using MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FacialRegionBox:\n",
    "    \"\"\"Represents a bounding box around a specific facial region.\"\"\"\n",
    "\n",
    "    min_x: int\n",
    "    min_y: int\n",
    "    max_x: int\n",
    "    max_y: int\n",
    "    region_name: str\n",
    "    importance_score: Optional[float] = None\n",
    "\n",
    "\n",
    "def _extract_region_boxes(landmarker: FaceLandmarker, region_map: dict[str, list[int]], image: np.ndarray) -> list[FacialRegionBox]:\n",
    "    \"\"\"Identify and create bounding boxes for each facial region in an image.\"\"\"\n",
    "    rgb_image = image.copy()\n",
    "    rgb_image = cv2.cvtColor((rgb_image * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    landmarks = _detect_facial_landmarks(rgb_image, landmarker)\n",
    "    if landmarks is None:\n",
    "        return {}\n",
    "\n",
    "    pixel_landmarks = _convert_to_pixel_coordinates(landmarks, image.shape)\n",
    "\n",
    "    regions = []\n",
    "    for region_name, landmark_ids in region_map.items():\n",
    "        region_points = [pixel_landmarks[i] for i in landmark_ids]\n",
    "        min_x = max(0, min(x for x, _ in region_points))\n",
    "        min_y = max(0, min(y for _, y in region_points))\n",
    "        max_x = max(x for x, _ in region_points)\n",
    "        max_y = max(y for _, y in region_points)\n",
    "        regions.append(FacialRegionBox(min_x, min_y, max_x, max_y, region_name))\n",
    "\n",
    "    return regions\n",
    "\n",
    "\n",
    "def identify_facial_regions(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Detect and store facial region boxes for each image in dataset.\"\"\"\n",
    "    landmarker = _load_face_landmarker()\n",
    "    region_map = _load_facial_region_map()\n",
    "\n",
    "    result = data.copy()\n",
    "    result[\"region_boxes\"] = result[\"image\"].progress_apply(lambda image: _extract_region_boxes(landmarker, region_map, image))\n",
    "    return result\n",
    "\n",
    "\n",
    "def visualize_region_boxes(data: pd.DataFrame, box_col: str, rows: int = 4, cols: int = 8, title: str = None) -> plt.Figure:\n",
    "    \"\"\"Display identified facial regions as colored bounding boxes.\"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # fmt: off\n",
    "    region_colors = {\n",
    "        \"left_eye\": (255, 0, 0),           # Red\n",
    "        \"right_eye\": (0, 255, 0),          # Green\n",
    "        \"nose\": (0, 0, 255),               # Blue\n",
    "        \"lips\": (255, 255, 0),             # Yellow\n",
    "        \"left_cheek\": (255, 0, 255),       # Magenta\n",
    "        \"right_cheek\": (0, 255, 255),      # Cyan\n",
    "        \"chin\": (255, 128, 0),             # Orange\n",
    "        \"forehead\": (128, 0, 255),         # Purple\n",
    "        \"left_eyebrow\": (0, 128, 128),     # Teal\n",
    "        \"right_eyebrow\": (128, 128, 0),    # Olive\n",
    "    }\n",
    "    # fmt: on\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(data):\n",
    "            image = data.iloc[i][\"image\"]\n",
    "            rgb_image = image.copy()\n",
    "            rgb_image = cv2.cvtColor((rgb_image * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "            boxes = data.iloc[i][box_col]\n",
    "            for box in boxes:\n",
    "                color = region_colors.get(box.region_name, (0, 0, 0))\n",
    "                cv2.rectangle(rgb_image, (box.min_x, box.min_y), (box.max_x, box.max_y), color, 1)\n",
    "            ax.imshow(rgb_image)\n",
    "            ax.axis(\"off\")\n",
    "        else:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "test_set = identify_facial_regions(test_set)\n",
    "fig = visualize_region_boxes(test_set, \"region_boxes\", title=\"Facial Region Boxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4.3. Filter landmarks based on overlap with activation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_region_importance(region_box: FacialRegionBox, activation_map: np.ndarray) -> float:\n",
    "    \"\"\"Calculate importance score of a facial region based on activation intensity.\"\"\"\n",
    "    roi = activation_map[\n",
    "        max(0, region_box.min_y) : min(activation_map.shape[0], region_box.max_y),\n",
    "        max(0, region_box.min_x) : min(activation_map.shape[1], region_box.max_x),\n",
    "    ]\n",
    "    return np.mean(roi) if roi.size != 0 else 0.0\n",
    "\n",
    "\n",
    "def _filter_regions_by_activation(region_boxes: list[FacialRegionBox], activation_map: np.ndarray) -> list[FacialRegionBox]:\n",
    "    \"\"\"Identify facial regions that have significant activation for gender prediction.\"\"\"\n",
    "    important_regions = []\n",
    "\n",
    "    for box in region_boxes:\n",
    "        box.importance_score = _compute_region_importance(box, activation_map)\n",
    "\n",
    "        if box.importance_score > ANALYSIS_FEATURE_IMPORTANCE_THRESHOLD:\n",
    "            important_regions.append(box)\n",
    "\n",
    "    return sorted(important_regions, key=lambda b: b.importance_score, reverse=True)\n",
    "\n",
    "\n",
    "def identify_important_regions(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Find and store facial regions with high importance for gender classification.\"\"\"\n",
    "    result = data.copy()\n",
    "\n",
    "    def process_row(row):\n",
    "        return _filter_regions_by_activation(row[\"region_boxes\"], row[\"activation_map\"])\n",
    "\n",
    "    result[\"important_regions\"] = result.progress_apply(process_row, axis=1)\n",
    "    return result\n",
    "\n",
    "\n",
    "test_set = identify_important_regions(test_set)\n",
    "fig = visualize_region_boxes(test_set, \"important_regions\", title=\"Important Facial Regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bias analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Compute feature bias scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_facial_feature_bias(data: pd.DataFrame, misclassified_only: bool = False) -> dict:\n",
    "    \"\"\"Return bias analysis for facial features from test_set.\"\"\"\n",
    "    subset = data[data[\"gender\"] != data[\"prediction\"]] if misclassified_only else data\n",
    "    features = _load_facial_region_map().keys()\n",
    "\n",
    "    total_male = (subset[\"gender\"] == 0).sum()\n",
    "    total_female = (subset[\"gender\"] == 1).sum()\n",
    "\n",
    "    feature_counts = {f: {\"male_count\": 0, \"female_count\": 0} for f in features}\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        regions = row.get(\"important_regions\")\n",
    "        if not isinstance(regions, list):\n",
    "            continue\n",
    "\n",
    "        present = {r.region_name for r in regions}\n",
    "\n",
    "        for f in present:\n",
    "            if row[\"gender\"] == 0:\n",
    "                feature_counts[f][\"male_count\"] += 1\n",
    "            elif row[\"gender\"] == 1:\n",
    "                feature_counts[f][\"female_count\"] += 1\n",
    "\n",
    "    def compute_stats(counts: dict[str, int]) -> dict[str, float]:\n",
    "        mp = round(counts[\"male_count\"] / total_male, 2) if total_male else 0.0\n",
    "        fp = round(counts[\"female_count\"] / total_female, 2) if total_female else 0.0\n",
    "        bs = round(abs(mp - fp), 2)\n",
    "        return {\"male_prob\": mp, \"female_prob\": fp, \"bias_score\": bs}\n",
    "\n",
    "    return {f: compute_stats(counts) for f, counts in feature_counts.items()}\n",
    "\n",
    "\n",
    "feature_biases = analyze_facial_feature_bias(test_set, misclassified_only=True)\n",
    "feature_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_bias(test_set: pd.DataFrame, misclassified_only: bool = False) -> plt.Figure:\n",
    "    \"\"\"Visualize male/female probabilities and bias scores for each facial feature.\"\"\"\n",
    "    bias_dict = analyze_facial_feature_bias(test_set, misclassified_only)\n",
    "\n",
    "    if not bias_dict:\n",
    "        raise ValueError(\"No feature bias data available to visualize.\")\n",
    "\n",
    "    df = pd.DataFrame.from_dict(bias_dict, orient=\"index\").reset_index().rename(columns={\"index\": \"feature\"})\n",
    "    df_melted = df.melt(id_vars=\"feature\", value_vars=[\"male_prob\", \"female_prob\"], var_name=\"gender\", value_name=\"probability\")\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    sns.barplot(data=df_melted, x=\"feature\", y=\"probability\", hue=\"gender\", ax=ax1)\n",
    "    ax1.set_ylabel(\"Probability\")\n",
    "    ax1.set_xlabel(\"Facial Feature\")\n",
    "    ax1.set_title(\"Facial Feature Probabilities and Bias Scores\")\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.pointplot(data=df, x=\"feature\", y=\"bias_score\", color=\"black\", ax=ax2, markers=\"D\", scale=1.2)\n",
    "    ax2.set_ylabel(\"Bias Score (Absolute Difference)\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ax1.legend(loc=\"upper center\", ncol=2)\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = visualize_feature_bias(test_set, misclassified_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Compute average bias score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_bias_score(test_set: pd.DataFrame, misclassified_only: bool = False) -> float:\n",
    "    \"\"\"Return the average bias score across all facial features.\"\"\"\n",
    "    feature_biases = analyze_facial_feature_bias(test_set, misclassified_only)\n",
    "    if not feature_biases:\n",
    "        return 0.0\n",
    "    return round(sum(stats[\"bias_score\"] for stats in feature_biases.values()) / len(feature_biases), 2)\n",
    "\n",
    "\n",
    "average_bias = compute_average_bias_score(test_set, misclassified_only=True)\n",
    "average_bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
