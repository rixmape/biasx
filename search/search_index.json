{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"BiasX <p>Explainable Gender Bias Analysis forFace Classification</p> Get Started View on GitHub bash $ pip install biasx 0% Feature-LevelBias Detection <p>Identify exactly which facial features contribute to gender misclassifications</p> ComprehensiveMetrics <p>Quantify bias through traditional fairness metrics and feature-based analyses</p> VisualExplanations <p>Generate heatmaps to reveal how facial features influence model predictions</p> ActionableInsights <p>Transform bias measurements into concrete model improvements</p>"},{"location":"#what-is-biasx","title":"What is BiasX?","text":"<p>BiasX is a comprehensive Python framework for detecting, measuring, and explaining gender bias in facial classification models. Unlike traditional fairness tools that only quantify bias through statistical metrics, BiasX reveals why bias occurs by connecting model decisions to specific facial features.</p> <p>By combining advanced visual explanation techniques like Grad-CAM with facial landmark detection, BiasX pinpoints which facial regions (eyes, nose, lips, etc.) disproportionately influence misclassifications across gender groups.</p> 1 Upload Model <p>Connect to your trained facial classification model</p> \u2192 2 Explain Predictions <p>Create activation maps and detect facial landmarks</p> \u2192 3 Quantify Bias <p>Measure feature-specific bias scores and disparities</p>"},{"location":"#why-use-biasx","title":"Why Use BiasX?Ready to understand your model's biases?","text":"<ul> <li>Explainable Metrics: Traditional fairness metrics tell you if bias exists, BiasX tells you why and where it appears.</li> <li>Actionable Insights: Pinpointing problematic features provides clear directions for model improvement.</li> <li>Interpretable Results: Visual explanations make bias findings accessible to both technical and non-technical stakeholders.</li> <li>Research Framework: Designed for both practical applications and academic research on algorithmic fairness.</li> </ul> Get Started View on GitHub"},{"location":"configuration/","title":"Configuration","text":"<p>BiasX uses a configuration system to manage parameters for datasets, models, explainers, and calculators. You can configure the <code>BiasAnalyzer</code> either by passing a Python dictionary directly during instantiation or by loading settings from a configuration file (commonly YAML, though JSON is also supported via utility functions).</p>"},{"location":"configuration/#configuration-structure","title":"Configuration Structure","text":"<p>The configuration is typically structured with top-level keys corresponding to the main components: <code>dataset</code>, <code>model</code>, <code>explainer</code>, and <code>calculator</code>.</p> <pre><code># Example Structure (Python Dictionary)\nconfig = {\n    \"dataset\": {\n        # Dataset parameters...\n    },\n    \"model\": {\n        # Model parameters...\n    },\n    \"explainer\": {\n        # Explainer parameters...\n    },\n    \"calculator\": {\n        # Calculator parameters...\n    },\n    \"analyzer\": { # Optional: Parameters directly for BiasAnalyzer itself\n        \"batch_size\": 32\n    }\n}\n\n# Example Instantiation\nfrom biasx import BiasAnalyzer\nanalyzer = BiasAnalyzer(config=config)\n\n# Example Instantiation from file\nanalyzer_from_file = BiasAnalyzer.from_file(\"path/to/your/config.yaml\") #\n</code></pre>"},{"location":"configuration/#component-configuration-details","title":"Component Configuration Details","text":"<p>Each component class uses the <code>@configurable</code> decorator, meaning its <code>__init__</code> parameters can be set via the configuration dictionary under the corresponding key (<code>dataset</code>, <code>model</code>, <code>explainer</code>, <code>calculator</code>).</p>"},{"location":"configuration/#dataset-dataset","title":"Dataset (<code>dataset</code>)","text":"<p>Parameters for the <code>biasx.datasets.Dataset</code> class.</p> <ul> <li><code>source</code>: (Required) The source dataset to use. Should correspond to values in the <code>DatasetSource</code> enum (e.g., <code>\"utkface\"</code>, <code>\"fairface\"</code>).</li> <li><code>image_width</code>: (Required) The target width to resize images to.</li> <li><code>image_height</code>: (Required) The target height to resize images to.</li> <li><code>color_mode</code>: (Required) The target color mode. Should correspond to values in the <code>ColorMode</code> enum (e.g., <code>\"L\"</code> for grayscale, <code>\"RGB\"</code> for color).</li> <li><code>max_samples</code>: (Optional) Maximum number of samples to load from the dataset. If <code>0</code> or less, all samples are loaded. Defaults depend on the implementation, but often it's useful to set a smaller number for testing (e.g., <code>100</code>).</li> <li><code>shuffle</code>: (Optional) Whether to shuffle the dataset before selecting <code>max_samples</code> or iterating. Defaults to <code>False</code>.</li> <li><code>seed</code>: (Optional) Random seed used for shuffling if <code>shuffle</code> is <code>True</code>. Defaults depend on implementation (e.g., <code>42</code>).</li> <li><code>batch_size</code>: (Optional) The number of images yielded per iteration when iterating over the dataset. Defaults depend on implementation (e.g., <code>32</code>).</li> </ul>"},{"location":"configuration/#model-model","title":"Model (<code>model</code>)","text":"<p>Parameters for the <code>biasx.models.Model</code> class.</p> <ul> <li><code>path</code>: (Required) Filesystem path to the saved Keras/TensorFlow model file (e.g., <code>.h5</code> or a SavedModel directory).</li> <li><code>inverted_classes</code>: (Required) Boolean indicating if the model's output class indices for Male/Female are inverted compared to the <code>Gender</code> enum (Male=0, Female=1). Set to <code>True</code> if your model predicts Female as 0 and Male as 1.</li> <li><code>batch_size</code>: (Optional) Batch size used for model prediction (<code>model.predict</code>). Defaults depend on implementation (e.g., <code>64</code>).</li> </ul>"},{"location":"configuration/#explainer-explainer","title":"Explainer (<code>explainer</code>)","text":"<p>Parameters for the <code>biasx.explainers.Explainer</code> class.</p> <ul> <li><code>landmarker_source</code>: (Required) Specifies the facial landmark detection model source. Should correspond to values in the <code>LandmarkerSource</code> enum (e.g., <code>\"mediapipe\"</code>).</li> <li><code>cam_method</code>: (Required) Specifies the Class Activation Map method to use. Should correspond to values in the <code>CAMMethod</code> enum (e.g., <code>\"gradcam\"</code>, <code>\"gradcam++\"</code>, <code>\"scorecam\"</code>).</li> <li><code>cutoff_percentile</code>: (Required) Integer percentile (0-100) used to threshold the raw activation map. Pixels below this percentile intensity are set to zero.</li> <li><code>threshold_method</code>: (Required) Specifies the method used to binarize the thresholded activation map to find distinct activation regions. Should correspond to values in the <code>ThresholdMethod</code> enum (e.g., <code>\"otsu\"</code>, <code>\"sauvola\"</code>, <code>\"triangle\"</code>).</li> <li><code>overlap_threshold</code>: (Required) Float value (0.0-1.0) determining the minimum overlap area (as a fraction of the activation box area) required to associate an activation box with a landmark box.</li> <li><code>distance_metric</code>: (Required) Specifies the distance metric used to find the nearest landmark center to an activation center. Should correspond to values in the <code>DistanceMetric</code> enum (e.g., <code>\"cityblock\"</code>, <code>\"cosine\"</code>, <code>\"euclidean\"</code>).</li> <li><code>batch_size</code>: (Optional) Batch size used during the explanation generation process (specifically for CAM generation). Defaults depend on implementation (e.g., <code>32</code>).</li> </ul>"},{"location":"configuration/#calculator-calculator","title":"Calculator (<code>calculator</code>)","text":"<p>Parameters for the <code>biasx.calculators.Calculator</code> class.</p> <ul> <li><code>precision</code>: (Required) Integer specifying the number of decimal places to round calculated bias scores and probabilities to.</li> </ul>"},{"location":"configuration/#analyzer-analyzer","title":"Analyzer (<code>analyzer</code>)","text":"<p>Optional parameters directly for the <code>biasx.analyzer.BiasAnalyzer</code> class itself.</p> <ul> <li><code>batch_size</code>: (Optional) Controls the batch size used when iterating through the dataset within the analyzer's main <code>analyze</code> loop. This is distinct from the dataset's own iteration batch size or the model/explainer batch sizes. Defaults depend on implementation (e.g., <code>32</code>).</li> </ul>"},{"location":"configuration/#example-configuration-file-configyaml","title":"Example Configuration File (<code>config.yaml</code>)","text":"<pre><code># Dataset Configuration\ndataset:\n  source: utkface         # Source dataset name (enum value)\n  max_samples: 5000      # Max images to load (0 for all)\n  shuffle: true          # Shuffle before selecting max_samples\n  seed: 42               # Random seed for shuffling\n  image_width: 48        # Target image width\n  image_height: 48       # Target image height\n  color_mode: \"L\"        # \"L\" for grayscale, \"RGB\" for color\n  batch_size: 64         # Batch size for dataset iteration\n\n# Model Configuration\nmodel:\n  path: \"./models/my_face_classifier.h5\" # Path to your trained model\n  inverted_classes: false              # Does your model output 0=Female, 1=Male?\n  batch_size: 128                      # Batch size for model.predict()\n\n# Explainer Configuration\nexplainer:\n  landmarker_source: mediapipe # Source for facial landmarks\n  cam_method: gradcam++        # CAM method (gradcam, gradcam++, scorecam)\n  cutoff_percentile: 95        # Percentile for CAM heatmap thresholding\n  threshold_method: otsu       # Method to binarize heatmap (otsu, sauvola, triangle)\n  overlap_threshold: 0.3       # Min overlap to link activation box to landmark\n  distance_metric: euclidean   # Metric for nearest landmark (cityblock, cosine, euclidean)\n  batch_size: 64               # Batch size for CAM generation\n\n# Calculator Configuration\ncalculator:\n  precision: 4               # Decimal places for calculated scores\n\n# Analyzer Configuration (Optional)\nanalyzer:\n  batch_size: 64             # Batch size for the main analysis loop\n</code></pre>"},{"location":"getting_started/","title":"Getting Started with BiasX","text":"<p>This guide provides a basic example of how to perform a gender bias analysis using the <code>BiasX</code> package.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed BiasX. If not, please follow the Installation guide.</p> <p>First, import the main analyzer class:</p> <pre><code>from biasx import BiasAnalyzer\nfrom biasx.types import DatasetSource, CAMMethod, LandmarkerSource # Import necessary enums\n</code></pre>"},{"location":"getting_started/#basic-analysis-workflow","title":"Basic Analysis Workflow","text":"<p>Here's a minimal example demonstrating the core steps:</p>"},{"location":"getting_started/#define-configuration","title":"Define Configuration","text":"<p>Instead of a separate file, we can define the configuration directly as a Python dictionary for simplicity. We'll use the UTKFace dataset, GradCAM for explanations, and limit the analysis to 100 samples for speed.</p> <pre><code># Define minimal configuration for the example\nconfig = {\n    \"dataset\": {\n        \"source\": DatasetSource.UTKFACE.value, # Specify the dataset source\n        \"max_samples\": 100,                   # Limit samples for a quick run\n        \"image_width\": 48,                    # Example image size\n        \"image_height\": 48,\n        \"color_mode\": \"L\",                  # Example: Use grayscale\n        \"batch_size\": 32                      # Example batch size\n    },\n    \"model\": {\n        \"path\": \"path/to/your/face_model.h5\", # IMPORTANT: Replace with the actual path to your Keras model\n        \"inverted_classes\": False,            # Set based on your model's output\n        \"batch_size\": 32                      # Model prediction batch size\n    },\n    \"explainer\": {\n        \"landmarker_source\": LandmarkerSource.MEDIAPIPE.value, # Use MediaPipe for landmarks\n        \"cam_method\": CAMMethod.GRADCAM.value,              # Use GradCAM for explanations\n        \"cutoff_percentile\": 90,                            # Example CAM cutoff\n        \"threshold_method\": \"otsu\",                         # Example thresholding\n        \"overlap_threshold\": 0.5,                           # Example overlap threshold\n        \"distance_metric\": \"euclidean\",                     # Example distance metric\n        \"batch_size\": 32                                    # Explainer processing batch size\n    },\n    \"calculator\": {\n        \"precision\": 4 # Example precision for calculations\n    }\n    # Output configuration defaults are often sufficient for getting started\n}\n</code></pre> <p>Note: Remember to replace <code>\"path/to/your/face_model.h5\"</code> with the actual path to your trained face classification model.</p>"},{"location":"getting_started/#instantiate-the-analyzer","title":"Instantiate the Analyzer","text":"<p>Create an instance of <code>BiasAnalyzer</code>, passing the configuration dictionary.</p> <pre><code>analyzer = BiasAnalyzer(config=config)\n</code></pre>"},{"location":"getting_started/#run-the-analysis","title":"Run the Analysis","text":"<p>Execute the full analysis pipeline. This will load the dataset, run predictions, generate explanations (landmarks and activation maps), and calculate bias metrics.</p> <pre><code>results = analyzer.analyze()\n</code></pre>"},{"location":"getting_started/#inspect-results","title":"Inspect Results","text":"<p>The <code>analyze</code> method returns an <code>AnalysisResult</code> object containing detailed findings. You can inspect overall scores or dive into feature-specific analyses.</p> <pre><code># Print the overall BiasX disparity score\nprint(f\"Overall BiasX Score: {results.disparity_scores.biasx}\")\n\n# Print the bias score calculated for a specific feature, e.g., the nose\nif \"nose\" in results.feature_analyses:\n        nose_analysis = results.feature_analyses[\"nose\"]\n        print(f\"Nose Bias Score: {nose_analysis.bias_score}\")\n        print(f\"  - Male Probability (Nose): {nose_analysis.male_probability}\")\n        print(f\"  - Female Probability (Nose): {nose_analysis.female_probability}\")\n\n# Explore detailed explanations for each image (optional)\n# for explanation in results.explanations:\n#    print(f\"Image ID: {explanation.image_data.image_id}, Predicted: {explanation.predicted_gender}\")\n#    # Access explanation.activation_boxes, explanation.landmark_boxes etc.\n</code></pre>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Configuration page for a detailed overview of all available settings.</li> <li>Dive into the specifics of each component in the API Reference.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>This guide will walk you through installing the <code>BiasX</code> package.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>Before installing BiasX, ensure you have the following prerequisites installed on your system:</p> <ul> <li>Python: A recent version of Python (e.g., 3.8 or later is recommended).</li> <li>pip: The Python package installer. This usually comes bundled with modern Python versions.</li> </ul> <p>You can check your installations by running:</p> <pre><code>python --version\npip --version\n</code></pre> <p>If you need to install Python, please visit python.org. If you need to install or upgrade pip, follow the instructions on the pip documentation.</p>"},{"location":"installation/#installing-biasx","title":"Installing BiasX","text":"<p>You can install the <code>BiasX</code> package directly from PyPI using pip:</p> <pre><code>pip install biasx\n</code></pre> <p>This command will download and install BiasX along with its necessary dependencies (such as TensorFlow, NumPy, MediaPipe, Hugging Face Hub, etc.).</p>"},{"location":"installation/#verifying-the-installation-optional","title":"Verifying the Installation (Optional)","text":"<p>To verify that BiasX was installed correctly, you can open a Python interpreter and try importing the main class:</p> <pre><code>try:\n    from biasx import BiasAnalyzer\n    print(\"BiasX installed successfully!\")\nexcept ImportError:\n    print(\"Error: BiasX not found. Please check your installation.\")\n</code></pre> <p>You are now ready to start using BiasX! Head over to the Getting Started guide for a basic usage example.</p>"},{"location":"api/analyzer/","title":"Bias Analyzer","text":"<p>This module contains the main orchestrator for the bias analysis pipeline.</p>"},{"location":"api/analyzer/#biasx.analyzer.BiasAnalyzer","title":"<code>biasx.analyzer.BiasAnalyzer</code>","text":"<p>Orchestrates the end-to-end facial recognition bias analysis pipeline.</p> <p>This class coordinates the process of loading data, running model inference, generating visual explanations (activation maps and facial landmarks), and calculating various bias metrics. It integrates functionalities from Dataset, Model, Explainer, and Calculator components.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Config</code> <p>The configuration object holding settings for all components (dataset, model, explainer, calculator).</p> <code>model</code> <code>Model</code> <p>An instance of the Model class for performing inference.</p> <code>dataset</code> <code>Dataset</code> <p>An instance of the Dataset class for loading and preprocessing data.</p> <code>explainer</code> <code>Explainer</code> <p>An instance of the Explainer class for generating visual explanations.</p> <code>calculator</code> <code>Calculator</code> <p>An instance of the Calculator class for computing bias metrics.</p> <code>batch_size</code> <code>int</code> <p>The batch size used for processing data during analysis, potentially overriding batch sizes specified in component configurations for the analysis loop itself.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Using a configuration dictionary\n&gt;&gt;&gt; config_dict = {\n...     \"dataset\": {\"source\": \"utkface\", \"max_samples\": 100},\n...     \"model\": {\"path\": \"/path/to/model.h5\"},\n...     \"explainer\": {\"cam_method\": \"gradcam\"},\n...     \"calculator\": {\"precision\": 4},\n...     \"analyzer\": {\"batch_size\": 16}\n... } #\n&gt;&gt;&gt; analyzer = BiasAnalyzer(config=config_dict)\n&gt;&gt;&gt; results = analyzer.analyze()\n&gt;&gt;&gt; print(results.disparity_scores)\n</code></pre> <pre><code>&gt;&gt;&gt; # Using a configuration file\n&gt;&gt;&gt; analyzer = BiasAnalyzer.from_file(\"config.yaml\") #\n&gt;&gt;&gt; results = analyzer.analyze()\n&gt;&gt;&gt; print(f\"BiasX Score: {results.disparity_scores.biasx}\")\n</code></pre>"},{"location":"api/analyzer/#biasx.analyzer.BiasAnalyzer.__init__","title":"<code>__init__(config=None, batch_size=32, **kwargs)</code>","text":"<p>Initializes the BiasAnalyzer and its components.</p> <p>Sets up the Dataset, Model, Explainer, and Calculator based on the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[Config, Dict, None]</code> <p>A configuration object (Config) or dictionary containing settings for the analyzer and its sub-components (Dataset, Model, Explainer, Calculator). If None, default configurations might be used or an error raised depending on the Config setup. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>The batch size to use when iterating through the dataset during the <code>analyze</code> method. This primarily controls the batching within the analyzer's loop, distinct from potential batch sizes used internally by the model or explainer if configured differently. Defaults to 32.</p> <code>32</code> <code>**kwargs</code> <p>Additional keyword arguments, potentially used by the configurable decorator or passed down during component initialization if the Config structure supports it.</p> <code>{}</code>"},{"location":"api/analyzer/#biasx.analyzer.BiasAnalyzer.analyze","title":"<code>analyze()</code>","text":"<p>Runs the full bias analysis pipeline on the configured dataset.</p> <p>This method iterates through the entire dataset provided by the Dataset component, processing images in batches using <code>analyze_batch</code>. It aggregates all the generated Explanation objects and then uses the Calculator component to compute feature-level bias analyses and overall disparity scores (like BiasX and Equalized Odds).</p> <p>Returns:</p> Type Description <code>AnalysisResult</code> <p>An AnalysisResult object containing:</p> <ul> <li><code>explanations</code>: A list of all Explanation objects generated for each image in the dataset.</li> <li><code>feature_analyses</code>: A dictionary mapping each FacialFeature to its calculated FeatureAnalysis (bias score, per-gender probabilities).</li> <li><code>disparity_scores</code>: A DisparityScores object containing overall metrics like BiasX and Equalized Odds.</li> </ul> <p>Returns an empty AnalysisResult if the dataset yields no data or no explanations could be generated.</p> Note <p>This method processes the entire dataset as configured in the Dataset component (respecting <code>max_samples</code>, shuffling, etc.). It can be computationally intensive depending on the dataset size and model complexity. It uses an internal buffer to manage memory usage during explanation aggregation.</p>"},{"location":"api/analyzer/#biasx.analyzer.BiasAnalyzer.analyze_batch","title":"<code>analyze_batch(image_data_batch)</code>","text":"<p>Analyzes a single batch of images through the pipeline.</p> <p>This method takes a list of ImageData objects, runs model prediction, generates explanations (activation maps, landmarks, labeled boxes), and compiles the results into Explanation objects.</p> <p>Parameters:</p> Name Type Description Default <code>image_data_batch</code> <code>List[ImageData]</code> <p>A list of ImageData objects, typically obtained from iterating over a Dataset instance. Each ImageData object should contain at least the preprocessed image (NumPy array) and the original PIL image.</p> required <p>Returns:</p> Type Description <code>List[Explanation]</code> <p>A list of Explanation objects, one for each image in the input batch. Each Explanation object contains the original image data, prediction results (gender, confidence), activation map, activation boxes (potentially labeled with facial features), and landmark boxes. Returns an empty list if the input batch is empty.</p>"},{"location":"api/analyzer/#biasx.analyzer.BiasAnalyzer.from_file","title":"<code>from_file(config_file_path)</code>  <code>classmethod</code>","text":"<p>Creates a BiasAnalyzer instance from a configuration file.</p> <p>This factory method provides a convenient way to initialize the analyzer using an external configuration file (e.g., YAML, JSON) that defines the settings for all components.</p> <p>Parameters:</p> Name Type Description Default <code>config_file_path</code> <code>str</code> <p>The path to the configuration file. The file format should be supported by the underlying Config class's <code>from_file</code> method.</p> required <p>Returns:</p> Type Description <code>BiasAnalyzer</code> <p>A new instance of BiasAnalyzer configured according to the file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; analyzer = BiasAnalyzer.from_file('analysis_config.yaml') #\n&gt;&gt;&gt; results = analyzer.analyze()\n</code></pre>"},{"location":"api/calculators/","title":"Calculators","text":"<p>This module provides classes for calculating bias metrics based on model explanations.</p>"},{"location":"api/calculators/#biasx.calculators.Calculator","title":"<code>biasx.calculators.Calculator</code>","text":"<p>Calculates bias metrics based on model explanations and ground truth.</p> <p>This class takes the explanations generated by the Explainer component (which include image data, predictions, and activation/landmark details) and computes quantitative measures of bias. It calculates feature-specific bias scores and overall disparity metrics like BiasX and Equalized Odds.</p> <p>Attributes:</p> Name Type Description <code>precision</code> <code>int</code> <p>The number of decimal places to round the calculated bias metrics to.</p>"},{"location":"api/calculators/#biasx.calculators.Calculator.__init__","title":"<code>__init__(precision, **kwargs)</code>","text":"<p>Initializes the bias Calculator.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>int</code> <p>The number of decimal places for rounding calculated bias scores and probabilities.</p> required <code>**kwargs</code> <p>Additional keyword arguments, potentially used by the configurable decorator or for future extensions.</p> <code>{}</code>"},{"location":"api/calculators/#biasx.calculators.Calculator.calculate_disparities","title":"<code>calculate_disparities(feature_analyses, explanations)</code>","text":"<p>Calculates overall disparity scores for the model.</p> <p>This method computes aggregate bias metrics based on the previously calculated feature-level analyses and the overall model performance across different gender groups.</p> <p>Parameters:</p> Name Type Description Default <code>feature_analyses</code> <code>Dict[FacialFeature, FeatureAnalysis]</code> <p>A dictionary mapping FacialFeatures to their corresponding FeatureAnalysis objects, as returned by <code>calculate_feature_biases</code>.</p> required <code>explanations</code> <code>List[Explanation]</code> <p>A list of Explanation objects for all analyzed images, needed to calculate performance metrics like Equalized Odds.</p> required <p>Returns:</p> Type Description <code>DisparityScores</code> <p>A DisparityScores object containing:</p> <ul> <li><code>biasx</code>: An overall bias score calculated as the average of the     absolute bias scores across all analyzed facial features.</li> <li><code>equalized_odds</code>: A score measuring the maximum disparity in     True Positive Rates (TPR) and False Positive Rates (FPR)     between male and female groups. A score of 0 indicates     perfect equality in error rates across genders.</li> </ul> <p>Returns default DisparityScores (often with 0 values) if no feature analyses are provided.</p>"},{"location":"api/calculators/#biasx.calculators.Calculator.calculate_feature_biases","title":"<code>calculate_feature_biases(explanations)</code>","text":"<p>Calculates bias metrics associated with each facial feature.</p> <p>This method analyzes how often different facial features are associated with model activations specifically during misclassifications for different gender groups (male vs. female). It determines the probability of a feature being activated when the model misclassifies an image of a certain true gender. The bias score for a feature reflects the absolute difference between these probabilities for males and females.</p> <p>Parameters:</p> Name Type Description Default <code>explanations</code> <code>List[Explanation]</code> <p>A list of Explanation objects, containing the detailed analysis results for each image processed.</p> required <p>Returns:</p> Type Description <code>Dict[FacialFeature, FeatureAnalysis]</code> <p>A dictionary where keys are FacialFeature enums and values are FeatureAnalysis objects. Each FeatureAnalysis object contains:</p> <ul> <li><code>feature</code>: The specific FacialFeature enum.</li> <li><code>bias_score</code>: The absolute difference between male and female     activation probabilities during misclassifications.</li> <li><code>male_probability</code>: The probability of this feature being     activated in misclassified male images.</li> <li><code>female_probability</code>: The probability of this feature being     activated in misclassified female images.</li> </ul> <p>Features with no observed activations during misclassifications are omitted from the result.</p>"},{"location":"api/datasets/","title":"Datasets","text":"<p>This module handles loading and preprocessing image datasets.</p>"},{"location":"api/datasets/#biasx.datasets.Dataset","title":"<code>biasx.datasets.Dataset</code>","text":"<p>Manages facial image datasets and preprocessing for model input.</p> <p>Loads dataset information from configuration, handles fetching dataset files (e.g., Parquet files from HuggingFace Hub), allows sampling and shuffling, and provides an iterator to yield batches of processed image data ready for model consumption.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>The identifier for the dataset source (e.g., 'utkface').</p> <code>image_width</code> <code>int</code> <p>The target width to resize images to.</p> <code>image_height</code> <code>int</code> <p>The target height to resize images to.</p> <code>color_mode</code> <code>ColorMode</code> <p>The target color mode ('L' for grayscale, 'RGB').</p> <code>single_channel</code> <code>bool</code> <p>Flag indicating if the output NumPy array should always have a single channel dimension (relevant for grayscale).</p> <code>max_samples</code> <code>int</code> <p>Maximum number of samples to load from the dataset. If &lt;= 0, all samples are loaded.</p> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the dataset before sampling or iteration.</p> <code>seed</code> <code>int</code> <p>Random seed used for shuffling if <code>shuffle</code> is True.</p> <code>batch_size</code> <code>int</code> <p>The number of images to yield in each batch during iteration.</p> <code>dataset_info</code> <code>ResourceMetadata</code> <p>Metadata loaded from configuration about the dataset resource (repo ID, filename, column names, etc.).</p> <code>dataset_path</code> <code>str</code> <p>The local path to the downloaded dataset file.</p> <code>dataframe</code> <code>DataFrame</code> <p>The pandas DataFrame holding the dataset metadata (after potential sampling and shuffling).</p>"},{"location":"api/datasets/#biasx.datasets.Dataset.__init__","title":"<code>__init__(source, image_width, image_height, color_mode, single_channel, max_samples, shuffle, seed, batch_size, **kwargs)</code>","text":"<p>Initialize the dataset with configuration and preprocessing parameters.</p> <p>Loads dataset metadata, potentially samples and shuffles the data based on configuration, and sets up image processing parameters.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Identifier for the dataset source (e.g., \"utkface\").           Must correspond to an entry in <code>dataset_config.json</code>.</p> required <code>image_width</code> <code>int</code> <p>Target width for images after resizing.</p> required <code>image_height</code> <code>int</code> <p>Target height for images after resizing.</p> required <code>color_mode</code> <code>ColorMode</code> <p>Target color mode for images                             (e.g., ColorMode.GRAYSCALE).</p> required <code>single_channel</code> <code>bool</code> <p>If True and color_mode is GRAYSCALE, ensures                    the preprocessed numpy array has a channel dim.</p> required <code>max_samples</code> <code>int</code> <p>The maximum number of samples to use from the dataset.                If 0 or negative, uses all samples.</p> required <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the dataset records.</p> required <code>seed</code> <code>int</code> <p>The random seed to use for shuffling if <code>shuffle</code> is True.</p> required <code>batch_size</code> <code>int</code> <p>The number of samples per batch for the iterator.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed via configuration.</p> <code>{}</code>"},{"location":"api/datasets/#biasx.datasets.Dataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate through the dataset, yielding batches of ImageData objects.</p> <p>Iterates over the internal DataFrame in steps of <code>self.batch_size</code>. For each batch, it loads the PIL images, preprocesses them into a NumPy array, extracts metadata, and then constructs a list of <code>ImageData</code> objects, where each object contains the ID, PIL image, preprocessed NumPy array slice, dimensions, and demographic labels for one sample.</p> <p>Yields:</p> Type Description <code>List[ImageData]</code> <p>A list of <code>biasx.types.ImageData</code> objects representing one batch of data. The list length will be equal to <code>self.batch_size</code>, except possibly for the last batch.</p>"},{"location":"api/datasets/#biasx.datasets.Dataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of images in the configured dataset.</p> <p>Returns the total number of samples in the DataFrame after loading, sampling, and shuffling have been applied.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of samples in the dataset.</p>"},{"location":"api/explainers/","title":"Explainers","text":"<p>This module provides classes for generating visual explanations like activation maps and detecting facial landmarks.</p>"},{"location":"api/explainers/#explainer","title":"Explainer","text":""},{"location":"api/explainers/#biasx.explainers.Explainer","title":"<code>biasx.explainers.Explainer</code>","text":"<p>Coordinates the generation of visual explanations for model decisions.</p> <p>This class integrates the <code>FacialLandmarker</code> and <code>ClassActivationMapper</code> to produce comprehensive explanations for a batch of images. It generates activation maps, processes them into activation boxes, detects landmark boxes, and attempts to label activation boxes based on their spatial overlap and proximity to landmark features.</p> <p>Attributes:</p> Name Type Description <code>landmarker</code> <code>FacialLandmarker</code> <p>An instance for detecting facial landmarks.</p> <code>activation_mapper</code> <code>ClassActivationMapper</code> <p>An instance for generating and processing activation maps.</p> <code>overlap_threshold</code> <code>float</code> <p>The minimum Intersection over Area (IoA) threshold required between an activation box and a landmark box for the activation box to inherit the landmark's feature label.</p> <code>distance_metric</code> <code>str</code> <p>The distance metric (e.g., 'euclidean', 'cityblock') used to find the nearest landmark box center for each activation box center.</p> <code>batch_size</code> <code>int</code> <p>The batch size hint used within this explainer, potentially influencing internal operations if implemented differently later. (Note: Current <code>explain_batch</code> processes the whole input batch at once).</p>"},{"location":"api/explainers/#biasx.explainers.Explainer.__init__","title":"<code>__init__(landmarker_source, cam_method, cutoff_percentile, threshold_method, overlap_threshold, distance_metric, batch_size, **kwargs)</code>","text":"<p>Initialize the visual explainer and its components.</p> <p>Creates instances of <code>FacialLandmarker</code> and <code>ClassActivationMapper</code> based on the provided configuration parameters. Stores thresholds and metrics used for associating activation boxes with features.</p> <p>Parameters:</p> Name Type Description Default <code>landmarker_source</code> <code>LandmarkerSource</code> <p>The source for the facial landmark model.</p> required <code>cam_method</code> <code>CAMMethod</code> <p>The class activation mapping method to use.</p> required <code>cutoff_percentile</code> <code>int</code> <p>The percentile threshold for heatmap processing.</p> required <code>threshold_method</code> <code>ThresholdMethod</code> <p>The binarization method for heatmap processing.</p> required <code>overlap_threshold</code> <code>float</code> <p>The IoA threshold (0.0 to 1.0) for labeling activation boxes based on landmark box overlap.</p> required <code>distance_metric</code> <code>DistanceMetric</code> <p>The metric for comparing box center distances.</p> required <code>batch_size</code> <code>int</code> <p>A batch size parameter (currently informational).</p> required <code>**kwargs</code> <p>Additional keyword arguments passed via configuration.</p> <code>{}</code>"},{"location":"api/explainers/#biasx.explainers.Explainer.explain_batch","title":"<code>explain_batch(pil_images, preprocessed_images, model, target_classes)</code>","text":"<p>Generate visual explanations for a batch of images.</p> <p>Orchestrates the explanation process for a batch:</p> <ol> <li>Generates activation maps using <code>ClassActivationMapper</code>.</li> <li>Processes maps into activation boxes using <code>ClassActivationMapper</code>.</li> <li>Detects landmark boxes using <code>FacialLandmarker</code>.</li> <li>For each image, attempts to assign a <code>FacialFeature</code> label to each    activation box by finding the nearest landmark box (based on center    distance) and checking if their spatial overlap (IoA) meets the    <code>overlap_threshold</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>pil_images</code> <code>List[Image]</code> <p>List of original PIL images in the batch.</p> required <code>preprocessed_images</code> <code>List[ndarray]</code> <p>List of corresponding preprocessed images (NumPy arrays) ready for model/CAM input.</p> required <code>model</code> <code>Model</code> <p>The model instance (needed by the activation mapper).</p> required <code>target_classes</code> <code>List[Gender]</code> <p>The target class (Gender) for generating the CAM for each corresponding image.</p> required <p>Returns:</p> Type Description <code>Tuple[List[ndarray], List[List[Box]], List[List[Box]]]</code> <p>A tuple containing three lists, all aligned with the input batch order:</p> <ul> <li>List[np.ndarray]: Raw activation maps (heatmaps) for each image.</li> <li>List[List[biasx.types.Box]]: Activation boxes for each image. Boxes   may have their <code>feature</code> attribute set if successfully labeled.</li> <li>List[List[biasx.types.Box]]: Landmark boxes for each image, with   their <code>feature</code> attribute set by the landmarker.</li> </ul> <p>Returns ([], [], []) if the input <code>pil_images</code> list is empty.</p>"},{"location":"api/explainers/#faciallandmarker","title":"FacialLandmarker","text":""},{"location":"api/explainers/#biasx.explainers.FacialLandmarker","title":"<code>biasx.explainers.FacialLandmarker</code>","text":"<p>Detects facial landmarks using MediaPipe.</p> <p>This class loads a pre-trained MediaPipe face landmark model, specified via configuration, and provides a method to detect landmarks in images. It maps the detected landmark points to specific facial features based on a predefined mapping file.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>LandmarkerSource</code> <p>The source identifier for the landmarker model used (e.g., MEDIAPIPE).</p> <code>landmarker_info</code> <code>ResourceMetadata</code> <p>Metadata loaded from configuration about the landmarker model resource.</p> <code>model_path</code> <code>str</code> <p>The local path to the downloaded landmarker model file.</p> <code>landmark_mapping</code> <code>Dict[FacialFeature, List[int]]</code> <p>A dictionary mapping facial features (enum) to lists of landmark indices provided by the MediaPipe model.</p> <code>detector</code> <code>FaceLandmarker</code> <p>The initialized MediaPipe FaceLandmarker instance.</p>"},{"location":"api/explainers/#biasx.explainers.FacialLandmarker.__init__","title":"<code>__init__(source)</code>","text":"<p>Initialize the facial landmark detector.</p> <p>Loads resources based on the specified source, sets up the MediaPipe FaceLandmarker options, and creates the detector instance.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>LandmarkerSource</code> <p>The source of the landmarker model to use (e.g., LandmarkerSource.MEDIAPIPE). Corresponds to keys in <code>landmarker_config.json</code>.</p> required"},{"location":"api/explainers/#biasx.explainers.FacialLandmarker.detect","title":"<code>detect(images)</code>","text":"<p>Detect facial landmarks in one or more images.</p> <p>Takes a single PIL image or a list of PIL images, converts them into MediaPipe's Image format, and runs detection using the initialized landmarker. For each image where landmarks are detected, it converts the normalized landmark coordinates to pixel coordinates and groups them into bounding boxes based on the <code>self.landmark_mapping</code> for each facial feature.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Union[Image, List[Image]]</code> <p>A single PIL image or a list of PIL images to process.</p> required <p>Returns:</p> Type Description <code>List[List[Box]]</code> <p>A list of lists of Box objects. The outer list corresponds to the input images. Each inner list contains <code>biasx.types.Box</code> objects, one for each facial feature defined in the mapping, representing the bounding box encompassing the landmarks for that feature in the corresponding image. If no landmarks are detected in an image, its corresponding inner list will be empty.</p>"},{"location":"api/explainers/#classactivationmapper","title":"ClassActivationMapper","text":""},{"location":"api/explainers/#biasx.explainers.ClassActivationMapper","title":"<code>biasx.explainers.ClassActivationMapper</code>","text":"<p>Generates and processes class activation maps (CAMs).</p> <p>This class uses a specified CAM generation method (e.g., Grad-CAM++, Score-CAM) from the <code>tf-keras-vis</code> library to produce heatmaps highlighting regions important for a model's classification decision. It also provides methods to process these heatmaps by applying thresholding and identifying contiguous activated regions as bounding boxes.</p> <p>Attributes:</p> Name Type Description <code>cam_method</code> <code>ModelVisualization</code> <p>The instantiated CAM visualization object (e.g., GradcamPlusPlus) based on the configured method.</p> <code>cutoff_percentile</code> <code>int</code> <p>The percentile value (0-100) used to determine the threshold for filtering low-activation areas in the heatmap.</p> <code>threshold_method</code> <code>Callable[[ndarray], Any]</code> <p>The thresholding function (e.g., skimage.filters.threshold_otsu) used to binarize the filtered heatmap.</p>"},{"location":"api/explainers/#biasx.explainers.ClassActivationMapper.__init__","title":"<code>__init__(cam_method, cutoff_percentile, threshold_method)</code>","text":"<p>Initialize the activation map generator and processor.</p> <p>Parameters:</p> Name Type Description Default <code>cam_method</code> <code>CAMMethod</code> <p>The CAM algorithm to use (e.g., CAMMethod.GRADCAM_PLUS_PLUS).</p> required <code>cutoff_percentile</code> <code>int</code> <p>The percentile (0-100) to use for thresholding the raw heatmap. Activations below this percentile are zeroed out.</p> required <code>threshold_method</code> <code>ThresholdMethod</code> <p>The algorithm used to binarize the filtered heatmap (e.g., ThresholdMethod.OTSU).</p> required"},{"location":"api/explainers/#biasx.explainers.ClassActivationMapper.generate_heatmap","title":"<code>generate_heatmap(model, preprocessed_images, target_classes)</code>","text":"<p>Generate class activation maps (heatmaps) for preprocessed images.</p> <p>Uses the configured CAM method (<code>tf-keras-vis</code>) to generate heatmaps for the given images with respect to the specified target classes. It handles preparing the images and defining the score function for the visualizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The trained Keras model to explain.</p> required <code>preprocessed_images</code> <code>Union[ndarray, List[ndarray]]</code> <p>A single preprocessed image (NumPy array) or a list/batch of preprocessed images (NumPy array). Assumes images are normalized and correctly shaped.</p> required <code>target_classes</code> <code>Union[Gender, List[Gender]]</code> <p>The target class(es) (Gender enum) for which to generate the heatmaps. If a single Gender is provided, it's used for all images in the batch.</p> required <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>A list of NumPy arrays, where each array is a 2D heatmap corresponding to an input image. Returns an empty list if the input is empty.</p>"},{"location":"api/explainers/#biasx.explainers.ClassActivationMapper.process_heatmap","title":"<code>process_heatmap(heatmaps, pil_images)</code>","text":"<p>Process heatmaps into bounding boxes of activated regions.</p> <p>Takes raw heatmaps, applies a percentile cutoff threshold, binarizes the result using the configured thresholding method, identifies connected regions (blobs) in the binary map, and converts these regions into bounding boxes scaled to the original image dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>heatmaps</code> <code>Union[ndarray, List[ndarray]]</code> <p>A single 2D heatmap or a list of 2D heatmaps (NumPy arrays) as generated by <code>generate_heatmap</code>.</p> required <code>pil_images</code> <code>List[Image]</code> <p>A list of the original PIL images corresponding to the heatmaps, used to get dimensions for scaling. Must be the same length as the list of heatmaps.</p> required <p>Returns:</p> Type Description <code>List[List[Box]]</code> <p>A list of lists of Box objects. The outer list corresponds to the input heatmaps/images. Each inner list contains <code>biasx.types.Box</code> objects representing the bounding boxes of activated regions found in the corresponding heatmap. Boxes do not have features assigned at this stage.</p>"},{"location":"api/models/","title":"Models","text":"<p>This module handles the loading and prediction using face classification models.</p>"},{"location":"api/models/#biasx.models.Model","title":"<code>biasx.models.Model</code>","text":"<p>Handles loading and inference for facial classification models.</p> <p>This class loads a Keras model from a specified path, handles batching and preprocessing of input images, performs inference, and processes the model's output probabilities into classified gender labels and confidence scores. It accounts for potentially inverted class labels.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <p>The loaded Keras model instance.</p> <code>inverted_classes</code> <code>bool</code> <p>If True, swaps the interpretation of the model's output classes (e.g., if the model outputs 0 for female and 1 for male, setting this to True maps 0 to MALE and 1 to FEMALE).</p> <code>batch_size</code> <code>int</code> <p>The batch size to use during model prediction (<code>model.predict</code>).</p> <code>_metadata</code> <code>Any</code> <p>Placeholder for potential future metadata storage.              Currently initialized to None and not used.</p>"},{"location":"api/models/#biasx.models.Model.__init__","title":"<code>__init__(path, inverted_classes, batch_size, **kwargs)</code>","text":"<p>Initialize the classification model handler.</p> <p>Loads the Keras model from the given file path and stores configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to the saved Keras model (.h5 or SavedModel dir).</p> required <code>inverted_classes</code> <code>bool</code> <p>Whether the model's output classes (0 and 1) should be interpreted in reverse order compared to the <code>Gender</code> enum (MALE=0, FEMALE=1).</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use when calling <code>model.predict</code>.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed via configuration.</p> <code>{}</code>"},{"location":"api/models/#biasx.models.Model.predict","title":"<code>predict(preprocessed_images)</code>","text":"<p>Make gender predictions from preprocessed images.</p> <p>This is the main public method for getting predictions. It orchestrates the process:</p> <ol> <li>Prepares the input images using <code>_prepare_input</code>.</li> <li>Gets raw probabilities from the model using <code>_get_probabilities</code>.</li> <li>Processes these probabilities into gender labels and confidences    using <code>_process_predictions</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>preprocessed_images</code> <code>Union[ndarray, List[ndarray]]</code> <p>A single preprocessed image, a list of images, or a batch of images (NumPy arrays).</p> required <p>Returns:</p> Type Description <code>List[Tuple[Gender, float]]</code> <p>A list of tuples, one for each input image. Each tuple contains: - predicted_gender (biasx.types.Gender): The predicted gender. - confidence (float): The prediction confidence.</p> <code>List[Tuple[Gender, float]]</code> <p>Returns an empty list if the input is empty.</p>"},{"location":"api/types/","title":"Data Types","text":"<p>This module defines common enumerations and data structures used throughout the BiasX library.</p>"},{"location":"api/types/#enumerations","title":"Enumerations","text":""},{"location":"api/types/#biasx.types.Gender","title":"<code>biasx.types.Gender</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Gender classification labels used in datasets and model outputs.</p> <p>Attributes:</p> Name Type Description <code>MALE</code> <p>Represents the male gender, typically assigned the integer value 0.</p> <code>FEMALE</code> <p>Represents the female gender, typically assigned the integer value 1.</p>"},{"location":"api/types/#biasx.types.Age","title":"<code>biasx.types.Age</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Age range classification labels, often used in datasets like UTKFace.</p> <p>Attributes:</p> Name Type Description <code>RANGE_0_9</code> <p>Age range 0-9 years.</p> <code>RANGE_10_19</code> <p>Age range 10-19 years.</p> <code>RANGE_20_29</code> <p>Age range 20-29 years.</p> <code>RANGE_30_39</code> <p>Age range 30-39 years.</p> <code>RANGE_40_49</code> <p>Age range 40-49 years.</p> <code>RANGE_50_59</code> <p>Age range 50-59 years.</p> <code>RANGE_60_69</code> <p>Age range 60-69 years.</p> <code>RANGE_70_PLUS</code> <p>Age range 70 years and above.</p>"},{"location":"api/types/#biasx.types.Race","title":"<code>biasx.types.Race</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Race classification labels used in datasets.</p> <p>Attributes:</p> Name Type Description <code>WHITE</code> <p>Represents the White race category.</p> <code>BLACK</code> <p>Represents the Black race category.</p> <code>ASIAN</code> <p>Represents the Asian race category.</p> <code>INDIAN</code> <p>Represents the Indian race category.</p> <code>OTHER</code> <p>Represents other race categories not listed.</p>"},{"location":"api/types/#biasx.types.FacialFeature","title":"<code>biasx.types.FacialFeature</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of facial features identifiable via landmark detection.</p> <p>Used to label landmark groups and potentially activation map regions.</p> <p>Attributes:</p> Name Type Description <code>LEFT_EYE</code> <p>The region corresponding to the left eye.</p> <code>RIGHT_EYE</code> <p>The region corresponding to the right eye.</p> <code>NOSE</code> <p>The region corresponding to the nose.</p> <code>LIPS</code> <p>The region corresponding to the lips.</p> <code>LEFT_CHEEK</code> <p>The region corresponding to the left cheek.</p> <code>RIGHT_CHEEK</code> <p>The region corresponding to the right cheek.</p> <code>CHIN</code> <p>The region corresponding to the chin.</p> <code>FOREHEAD</code> <p>The region corresponding to the forehead.</p> <code>LEFT_EYEBROW</code> <p>The region corresponding to the left eyebrow.</p> <code>RIGHT_EYEBROW</code> <p>The region corresponding to the right eyebrow.</p>"},{"location":"api/types/#biasx.types.DatasetSource","title":"<code>biasx.types.DatasetSource</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Identifiers for supported dataset sources.</p> <p>Used in configuration to specify which dataset to load.</p> <p>Attributes:</p> Name Type Description <code>UTKFACE</code> <p>Represents the UTKFace dataset.</p> <code>FAIRFACE</code> <p>Represents the FairFace dataset.</p>"},{"location":"api/types/#biasx.types.LandmarkerSource","title":"<code>biasx.types.LandmarkerSource</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Identifiers for supported facial landmark detection models/providers.</p> <p>Used in configuration to specify the landmarker implementation.</p> <p>Attributes:</p> Name Type Description <code>MEDIAPIPE</code> <p>Represents the MediaPipe face landmarker model.</p>"},{"location":"api/types/#biasx.types.ColorMode","title":"<code>biasx.types.ColorMode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Image color modes compatible with PIL (Pillow).</p> <p>Used in dataset configuration to specify target image format.</p> <p>Attributes:</p> Name Type Description <code>GRAYSCALE</code> <p>Represents grayscale ('L' mode in PIL).</p> <code>RGB</code> <p>Represents standard Red-Green-Blue color ('RGB' mode in PIL).</p>"},{"location":"api/types/#biasx.types.CAMMethod","title":"<code>biasx.types.CAMMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Supported Class Activation Mapping (CAM) methods.</p> <p>Used in configuration to select the algorithm for generating visual explanations (heatmaps).</p> <p>Attributes:</p> Name Type Description <code>GRADCAM</code> <p>Represents the Grad-CAM algorithm.</p> <code>GRADCAM_PLUS_PLUS</code> <p>Represents the Grad-CAM++ algorithm.</p> <code>SCORECAM</code> <p>Represents the Score-CAM algorithm.</p>"},{"location":"api/types/#biasx.types.CAMMethod.get_implementation","title":"<code>get_implementation()</code>","text":"<p>Get the implementation class for this CAM method.</p>"},{"location":"api/types/#biasx.types.ThresholdMethod","title":"<code>biasx.types.ThresholdMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Supported thresholding methods for processing activation maps.</p> <p>Used in configuration to select the algorithm for binarizing heatmaps after initial percentile filtering. Relies on <code>skimage.filters</code>.</p> <p>Attributes:</p> Name Type Description <code>OTSU</code> <p>Represents Otsu's thresholding method.</p> <code>SAUVOLA</code> <p>Represents Sauvola's thresholding method (local).</p> <code>TRIANGLE</code> <p>Represents the Triangle thresholding method.</p>"},{"location":"api/types/#biasx.types.ThresholdMethod.get_implementation","title":"<code>get_implementation()</code>","text":"<p>Get the corresponding implementation function from <code>skimage.filters</code>.</p> <p>Returns the specific thresholding function (e.g., <code>threshold_otsu</code>) associated with the enum member.</p> <p>Returns:</p> Type Description <code>Callable[[ndarray], Any]</code> <p>The <code>skimage.filters</code> function implementing the selected method.</p>"},{"location":"api/types/#biasx.types.DistanceMetric","title":"<code>biasx.types.DistanceMetric</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Supported distance metrics for comparing spatial coordinates.</p> <p>Used in configuration to specify how the distance between activation box centers and landmark box centers is calculated. Values correspond to valid metrics for <code>scipy.spatial.distance.cdist</code>.</p> <p>Attributes:</p> Name Type Description <code>CITYBLOCK</code> <p>Represents the Manhattan distance (L1 norm).</p> <code>COSINE</code> <p>Represents the Cosine distance.</p> <code>EUCLIDEAN</code> <p>Represents the standard Euclidean distance (L2 norm).</p>"},{"location":"api/types/#data-classes","title":"Data Classes","text":""},{"location":"api/types/#biasx.types.Box","title":"<code>biasx.types.Box</code>  <code>dataclass</code>","text":"<p>Represents a rectangular bounding box with an optional feature label.</p> <p>Used for both facial landmark features and activation map regions.</p> <p>Attributes:</p> Name Type Description <code>min_x</code> <code>int</code> <p>The minimum x-coordinate (left edge) of the box.</p> <code>min_y</code> <code>int</code> <p>The minimum y-coordinate (top edge) of the box.</p> <code>max_x</code> <code>int</code> <p>The maximum x-coordinate (right edge) of the box.</p> <code>max_y</code> <code>int</code> <p>The maximum y-coordinate (bottom edge) of the box.</p> <code>feature</code> <code>Optional[FacialFeature]</code> <p>The facial feature associated with this box, if identified (e.g., FacialFeature.NOSE). Defaults to None.</p>"},{"location":"api/types/#biasx.types.Box.area","title":"<code>area</code>  <code>property</code>","text":"<p>Compute area of the box.</p>"},{"location":"api/types/#biasx.types.Box.center","title":"<code>center</code>  <code>property</code>","text":"<p>Compute center coordinates of the box.</p>"},{"location":"api/types/#biasx.types.ResourceMetadata","title":"<code>biasx.types.ResourceMetadata</code>  <code>dataclass</code>","text":"<p>Metadata for a resource, typically downloaded from HuggingFace Hub.</p> <p>Used to store information about datasets and models defined in configuration files.</p> <p>Attributes:</p> Name Type Description <code>repo_id</code> <code>str</code> <p>The repository ID on HuggingFace Hub (e.g., 'janko/utkface-dataset').</p> <code>filename</code> <code>str</code> <p>The specific filename within the repository (e.g., 'utkface_aligned_cropped.parquet').</p> <code>repo_type</code> <code>str</code> <p>The type of repository on HuggingFace Hub (e.g., 'dataset', 'model').              Defaults to 'dataset'.</p> <code>image_id_col</code> <code>str</code> <p>The name of the column containing image identifiers in a dataset.                 Defaults to \"\". Relevant only for datasets.</p> <code>image_col</code> <code>str</code> <p>The name of the column containing image data (e.g., bytes) in a dataset.              Defaults to \"\". Relevant only for datasets.</p> <code>gender_col</code> <code>str</code> <p>The name of the column containing gender labels in a dataset.               Defaults to \"\". Relevant only for datasets.</p> <code>age_col</code> <code>str</code> <p>The name of the column containing age labels in a dataset.            Defaults to \"\". Relevant only for datasets.</p> <code>race_col</code> <code>str</code> <p>The name of the column containing race labels in a dataset.             Defaults to \"\". Relevant only for datasets.</p>"},{"location":"api/types/#biasx.types.ImageData","title":"<code>biasx.types.ImageData</code>  <code>dataclass</code>","text":"<p>Container for data associated with a single image sample.</p> <p>Includes identifiers, raw and preprocessed image representations, dimensions, and demographic labels.</p> <p>Attributes:</p> Name Type Description <code>image_id</code> <code>str</code> <p>A unique identifier for the image.</p> <code>pil_image</code> <code>Optional[Image]</code> <p>The raw image loaded as a PIL object.                                Defaults to None.</p> <code>preprocessed_image</code> <code>Optional[ndarray]</code> <p>The image after numerical preprocessing (e.g., resizing, normalization, type conversion), ready for model input. Defaults to None.</p> <code>width</code> <code>Optional[int]</code> <p>The width of the <code>preprocessed_image</code>. Defaults to None.</p> <code>height</code> <code>Optional[int]</code> <p>The height of the <code>preprocessed_image</code>. Defaults to None.</p> <code>gender</code> <code>Optional[Gender]</code> <p>The ground truth gender label for the image.                        Defaults to None.</p> <code>age</code> <code>Optional[Age]</code> <p>The ground truth age label for the image. Defaults to None.</p> <code>race</code> <code>Optional[Race]</code> <p>The ground truth race label for the image. Defaults to None.</p>"},{"location":"api/types/#biasx.types.Explanation","title":"<code>biasx.types.Explanation</code>  <code>dataclass</code>","text":"<p>Container for the analysis results and explanations for a single image.</p> <p>Combines the input image data with model predictions and visual explanation outputs (activation maps, boxes).</p> <p>Attributes:</p> Name Type Description <code>image_data</code> <code>ImageData</code> <p>The original data associated with the image.</p> <code>predicted_gender</code> <code>Gender</code> <p>The gender predicted by the model.</p> <code>prediction_confidence</code> <code>float</code> <p>The model's confidence score for the prediction.</p> <code>activation_map</code> <code>ndarray</code> <p>The raw 2D heatmap generated by the CAM method.</p> <code>activation_boxes</code> <code>List[Box]</code> <p>Bounding boxes derived from the activation map.                           Boxes may have their <code>feature</code> attribute set if linked                           to a landmark.</p> <code>landmark_boxes</code> <code>List[Box]</code> <p>Bounding boxes detected for facial landmarks,                         with their <code>feature</code> attribute set.</p>"},{"location":"api/types/#biasx.types.FeatureAnalysis","title":"<code>biasx.types.FeatureAnalysis</code>  <code>dataclass</code>","text":"<p>Container for bias analysis results specific to a single facial feature.</p> <p>Stores the calculated probability of a feature being activated during misclassifications for each gender group, and the resulting bias score.</p> <p>Attributes:</p> Name Type Description <code>feature</code> <code>FacialFeature</code> <p>The facial feature being analyzed.</p> <code>bias_score</code> <code>float</code> <p>The absolute difference between <code>male_probability</code> and                 <code>female_probability</code>. A measure of bias associated                 with this feature.</p> <code>male_probability</code> <code>float</code> <p>The probability that this feature was activated                       in images where the true gender was male, but the                       model predicted incorrectly.</p> <code>female_probability</code> <code>float</code> <p>The probability that this feature was activated                         in images where the true gender was female, but the                         model predicted incorrectly.</p>"},{"location":"api/types/#biasx.types.DisparityScores","title":"<code>biasx.types.DisparityScores</code>  <code>dataclass</code>","text":"<p>Container for overall bias and fairness disparity metrics for the model.</p> <p>Aggregates results across features and performance metrics.</p> <p>Attributes:</p> Name Type Description <code>biasx</code> <code>float</code> <p>An overall bias score, calculated as the average of the            absolute <code>bias_score</code> values across all analyzed features            in <code>FeatureAnalysis</code>. Defaults to 0.0.</p> <code>equalized_odds</code> <code>float</code> <p>A fairness metric representing the maximum disparity                     between male and female groups in either the True                     Positive Rate (TPR) or the False Positive Rate (FPR).                     A score of 0 indicates perfect equality in these error rates.                     Defaults to 0.0.</p>"},{"location":"api/types/#biasx.types.AnalysisResult","title":"<code>biasx.types.AnalysisResult</code>  <code>dataclass</code>","text":"<p>Container for the complete results of a bias analysis run.</p> <p>Holds all generated explanations, feature-specific analyses, and overall disparity scores.</p> <p>Attributes:</p> Name Type Description <code>explanations</code> <code>List[Explanation]</code> <p>A list containing an <code>Explanation</code> object for every image analyzed in the run. Defaults to an empty list.</p> <code>feature_analyses</code> <code>Dict[FacialFeature, FeatureAnalysis]</code> <p>A dictionary mapping each analyzed <code>FacialFeature</code> to its corresponding <code>FeatureAnalysis</code> object. Defaults to an empty dictionary.</p> <code>disparity_scores</code> <code>DisparityScores</code> <p>An object containing the calculated overall bias metrics (<code>biasx</code>, <code>equalized_odds</code>). Defaults to a <code>DisparityScores</code> object with default (0.0) values.</p>"},{"location":"api/utils/","title":"Utilities","text":"<p>This module provides common utility functions used within the BiasX library.</p>"},{"location":"api/utils/#biasx.utils.get_json_config","title":"<code>biasx.utils.get_json_config(caller_file, config_file)</code>  <code>cached</code>","text":"<p>Load a JSON configuration file relative to the calling module's data directory.</p> <p>Constructs the path to the config file assuming it resides within a 'data' subdirectory relative to the Python file that calls this function. Uses LRU caching to avoid reloading the same configuration file multiple times.</p> <p>Parameters:</p> Name Type Description Default <code>caller_file</code> <code>str</code> <p>The path to the Python file calling this function                (typically <code>__file__</code> from the caller). Used to determine                the module's directory.</p> required <code>config_file</code> <code>str</code> <p>The name of the JSON configuration file (e.g.,                'dataset_config.json').</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the parsed JSON configuration data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the constructed path to the configuration file                does not exist.</p> <code>JSONDecodeError</code> <p>If the file exists but is not valid JSON.</p>"},{"location":"api/utils/#biasx.utils.get_resource_path","title":"<code>biasx.utils.get_resource_path(repo_id, filename, repo_type='dataset', force_download=False)</code>  <code>cached</code>","text":"<p>Download or retrieve a cached resource file from HuggingFace Hub.</p> <p>Uses the <code>huggingface_hub</code> library to download a file from a specified repository. Manages caching in a standardized local directory structure within <code>~/.biasx/cache/</code>. Uses LRU caching on the function call itself to quickly return the path if requested again with the same arguments.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>The HuggingFace Hub repository ID (e.g., 'google/mediapipe').</p> required <code>filename</code> <code>str</code> <p>The name of the file to download from the repository.</p> required <code>repo_type</code> <code>str</code> <p>The type of the repository (e.g., 'dataset',                        'model', 'space'). Defaults to \"dataset\".</p> <code>'dataset'</code> <code>force_download</code> <code>bool</code> <p>If True, forces re-downloading the file                              even if it exists in the cache. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The local file path to the downloaded or cached resource.</p>"},{"location":"api/utils/#biasx.utils.get_cache_dir","title":"<code>biasx.utils.get_cache_dir(name)</code>","text":"<p>Get or create a standardized cache directory path for persistent storage.</p> <p>Constructs a path within the user's home directory under <code>.biasx/cache/</code>. Creates the directory (including parent directories) if it doesn't exist. Used by <code>get_resource_path</code> to determine where to store downloaded files.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A specific name for the subdirectory within the cache         (e.g., the <code>repo_id</code> with slashes replaced).</p> required <p>Returns:</p> Type Description <code>Path</code> <p>A <code>pathlib.Path</code> object representing the absolute path to the</p> <code>Path</code> <p>specific cache subdirectory.</p>"},{"location":"api/utils/#biasx.utils.get_file_path","title":"<code>biasx.utils.get_file_path(caller_file, path)</code>","text":"<p>Get the absolute path to a file relative to the calling module.</p> <p>Resolves a relative path based on the directory containing the Python file that calls this function. Checks if the resulting file path exists.</p> <p>Parameters:</p> Name Type Description Default <code>caller_file</code> <code>str</code> <p>The path to the Python file calling this function                (typically <code>__file__</code> from the caller).</p> required <code>path</code> <code>str</code> <p>The relative path from the <code>caller_file</code>'s directory to the         target file (e.g., 'data/landmark_mapping.json').</p> required <p>Returns:</p> Type Description <code>Path</code> <p>A <code>pathlib.Path</code> object representing the absolute path to the target file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the resolved file path does not exist.</p>"}]}