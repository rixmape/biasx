"""Bias calculation module for BiasX."""

from typing import Dict, List, Tuple

from sklearn.metrics import confusion_matrix

from .config import configurable
from .types import DisparityScores, Explanation, FacialFeature, FeatureAnalysis, Gender


@configurable("calculator")
class Calculator:
    """Calculates bias metrics based on model explanations and ground truth.

    This class takes the explanations generated by the Explainer component
    (which include image data, predictions, and activation/landmark details)
    and computes quantitative measures of bias. It calculates feature-specific
    bias scores and overall disparity metrics like BiasX and Equalized Odds.

    Attributes:
        precision (int): The number of decimal places to round the calculated
            bias metrics to.
    """

    def __init__(self, precision: int, **kwargs):
        """Initializes the bias Calculator.

        Args:
            precision (int): The number of decimal places for rounding calculated
                bias scores and probabilities.
            **kwargs: Additional keyword arguments, potentially used by the
                configurable decorator or for future extensions.
        """
        self.precision = precision

    def calculate_feature_biases(self, explanations: List[Explanation]) -> Dict[FacialFeature, FeatureAnalysis]:
        """Calculates bias metrics associated with each facial feature.

        This method analyzes how often different facial features are associated
        with model activations specifically during misclassifications for
        different gender groups (male vs. female). It determines the probability
        of a feature being activated when the model misclassifies an image of
        a certain true gender. The bias score for a feature reflects the
        absolute difference between these probabilities for males and females.

        Args:
            explanations (List[biasx.types.Explanation]): A list of Explanation
                objects, containing the detailed analysis results for each image
                processed.

        Returns:
            A dictionary where keys are FacialFeature enums and values are
                FeatureAnalysis objects. Each FeatureAnalysis object contains:

                - `feature`: The specific FacialFeature enum.
                - `bias_score`: The absolute difference between male and female
                    activation probabilities during misclassifications.
                - `male_probability`: The probability of this feature being
                    activated in misclassified male images.
                - `female_probability`: The probability of this feature being
                    activated in misclassified female images.

                Features with no observed activations during misclassifications
                are omitted from the result.
        """
        feature_map = self._get_feature_activation_map(explanations)

        return {
            feature: FeatureAnalysis(
                feature=feature,
                bias_score=round(abs(probs[Gender.MALE] - probs[Gender.FEMALE]), self.precision),
                male_probability=probs[Gender.MALE],
                female_probability=probs[Gender.FEMALE],
            )
            for feature, probs in feature_map.items()
            if feature is not None
        }

    def calculate_disparities(self, feature_analyses: Dict[FacialFeature, FeatureAnalysis], explanations: List[Explanation]) -> DisparityScores:
        """Calculates overall disparity scores for the model.

        This method computes aggregate bias metrics based on the previously
        calculated feature-level analyses and the overall model performance
        across different gender groups.

        Args:
            feature_analyses (Dict[biasx.types.FacialFeature, biasx.types.FeatureAnalysis]):
                A dictionary mapping FacialFeatures to their corresponding
                FeatureAnalysis objects, as returned by `calculate_feature_biases`.
            explanations (List[biasx.types.Explanation]): A list of
                Explanation objects for all analyzed images, needed to
                calculate performance metrics like Equalized Odds.

        Returns:
            A DisparityScores object containing:

                - `biasx`: An overall bias score calculated as the average of the
                    absolute bias scores across all analyzed facial features.
                - `equalized_odds`: A score measuring the maximum disparity in
                    True Positive Rates (TPR) and False Positive Rates (FPR)
                    between male and female groups. A score of 0 indicates
                    perfect equality in error rates across genders.

                Returns default DisparityScores (often with 0 values) if no
                feature analyses are provided.
        """
        if not feature_analyses:
            return DisparityScores()

        bias_scores = [analysis.bias_score for analysis in feature_analyses.values()]
        biasx_score = round(sum(bias_scores) / len(bias_scores), self.precision)
        equalized_odds_score = self._calculate_equalized_odds_score(explanations)

        return DisparityScores(biasx=biasx_score, equalized_odds=equalized_odds_score)

    def _calculate_equalized_odds_score(self, explanations: List[Explanation]) -> float:
        """Calculates the equalized odds score based on prediction disparities.

        This score measures the fairness of the model in terms of error rates
        across different sensitive groups (here, gender). It computes the
        True Positive Rate (TPR) and False Positive Rate (FPR) separately for
        male and female subgroups based on the ground truth labels and model
        predictions provided in the explanations. The score is the maximum of
        the absolute difference in TPRs and the absolute difference in FPRs
        between the two groups.

        Args:
            explanations (List[biasx.types.Explanation]): A list of Explanation
                objects containing ground truth gender and predicted gender
                for each image.

        Returns:
            The calculated equalized odds score, rounded to the specified
            precision. A value of 0 indicates perfect equalized odds.
        """
        y_true_female, y_pred_female = self._get_gender_predictions(explanations, Gender.FEMALE)
        y_true_male, y_pred_male = self._get_gender_predictions(explanations, Gender.MALE)

        tn_f, fp_f, fn_f, tp_f = confusion_matrix(y_true_female, y_pred_female).ravel()
        tn_m, fp_m, fn_m, tp_m = confusion_matrix(y_true_male, y_pred_male).ravel()

        tpr_female = tp_f / (tp_f + fn_f) if (tp_f + fn_f) > 0 else 0.0
        fpr_female = fp_f / (tn_f + fp_f) if (tn_f + fp_f) > 0 else 0.0

        tpr_male = tp_m / (tp_m + fn_m) if (tp_m + fn_m) > 0 else 0.0
        fpr_male = fp_m / (tn_m + fp_m) if (tn_m + fp_m) > 0 else 0.0

        tpr_disparity = abs(tpr_female - tpr_male)
        fpr_disparity = abs(fpr_female - fpr_male)

        return round(max(tpr_disparity, fpr_disparity), self.precision)

    def _get_gender_predictions(self, explanations: List[Explanation], gender: Gender) -> Tuple[List[int], List[int]]:
        """Extracts binary ground truth and predictions for a specific gender group.

        Filters the explanations to consider only samples belonging to the
        specified true `gender`. It then creates two lists: one representing
        the binary ground truth (1 if the sample matches the target `gender`,
        0 otherwise - though all should be 1 in practice here after filtering,
        this structure allows reuse) and another representing the binary
        prediction (1 if the model predicted the target `gender`, 0 otherwise).
        This is primarily used for calculating confusion matrices for specific
        subgroups.

        Args:
            explanations (List[biasx.types.Explanation]): The list of all Explanation objects.
            gender (biasx.types.Gender): The target Gender (MALE or FEMALE) to extract predictions for.

        Returns:
            A tuple containing two lists:
                - y_true: A list of binary ground truth labels (1s and 0s).
                - y_pred: A list of binary prediction labels (1s and 0s).
        """
        y_true = []
        y_pred = []

        for explanation in explanations:
            is_target_gender = explanation.image_data.gender == gender
            y_true.append(1 if is_target_gender else 0)
            y_pred.append(1 if explanation.predicted_gender == gender else 0)

        return y_true, y_pred

    def _get_feature_activation_map(self, explanations: List[Explanation]) -> Dict[FacialFeature, Dict[Gender, float]]:
        """Maps features to their activation probability during misclassifications.

        Iterates through explanations, focusing only on misclassified samples.
        For each misclassified sample, it counts which facial features were
        activated (based on labeled activation boxes). It aggregates these counts
        separately for misclassified males and misclassified females. Finally, it
        calculates the probability of each feature being activated given a
        misclassification of a specific gender (count / total misclassifications
        for that gender).

        Args:
            explanations (List[biasx.types.Explanation]): A list of all Explanation objects.

        Returns:
            A dictionary where keys are FacialFeature enums. Values are nested
            dictionaries mapping Gender (MALE, FEMALE) to the calculated
            probability (float) that the feature was activated when an image of
            that gender was misclassified. Features that were never activated
            during any misclassification are excluded.
        """
        feature_activations = {feature: {Gender.MALE: 0, Gender.FEMALE: 0} for feature in FacialFeature}
        misclassified_counts = {Gender.MALE: 0, Gender.FEMALE: 0}

        for explanation in explanations:
            true_gender = explanation.image_data.gender
            predicted_gender = explanation.predicted_gender

            if true_gender != predicted_gender:
                misclassified_counts[true_gender] += 1
                activated_features = {box.feature for box in explanation.activation_boxes if box.feature}

                for feature in activated_features:
                    feature_activations[feature][true_gender] += 1

        # Calculate probabilities for each feature
        result = {}
        for feature, counts in feature_activations.items():
            probs = {}
            for gender in [Gender.MALE, Gender.FEMALE]:
                if misclassified_counts[gender] == 0:
                    probs[gender] = 0.0
                else:
                    probs[gender] = round(counts[gender] / misclassified_counts[gender], self.precision)

            if any(probs.values()):  # Only include features with activations
                result[feature] = probs

        return result
